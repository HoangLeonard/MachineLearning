{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mô hình hồi quy logistic\n",
    "σ(z) = θ.x \n",
    "\n",
    "Mô hình hồi quy logistic có thể coi là một mạng neural chỉ có một neural với đầu ra y = σ(θ.x) ∈ [0,1]\n",
    "\n",
    "**=> một mắt xích neural có dạng input -> sum/activation function -> output.**\n",
    "\n",
    "Một hàm activation thường gặp là RELU.\n",
    "<div style=\"text-align: center\">\n",
    "σ(z) = z if z>0 then 0.\n",
    "</div>\n",
    "Bây giờ nếu ta có 2 neural nhận input đầu vào, cách tính toán cũng như vậy tương tự với neural còn lại. Khi đó ta cần 1 neural nữa để tổng hợp kết quả từ 2 unit này. Khi này mạng neural của chúng ta sẽ có 3 tầng (layer). Lưu ý các tầng luôn được thêm bias để xấp xỉ một mạng neural.\n",
    "\n",
    "Áp dụng công thức ta thu được các phép toán tính toán trong mỗi lớp như sau:\n",
    "\n",
    "Với $\\theta_{ji}^{(k)}$ là trọng số của neural đi từ j đến i của layer k \n",
    "\n",
    "# Fit forward:\n",
    " \n",
    "* Layer 1:\n",
    "$$z_1^{(1)} = \\sum^{D}_{j=0}\\theta_{1j}^{(1)}*x_j$$\n",
    "$$a_1^{(1)} = \\sigma(z_1^{(1)})$$\n",
    "\n",
    "$$z_2^{(1)} = \\sum^{D}_{j=0}\\theta_{2j}^{(1)}*x_j$$\n",
    "$$a_2^{(1)} = \\sigma(z_2^{(1)})$$\n",
    "* Layer 2:\n",
    "$$z_2^{(2)} = \\sum^{D}_{j=0}\\theta_{1j}^{(2)}*a_j^{(1)}$$\n",
    "$$a_2^{(2)} = \\sigma(z_2^{(2)})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong lập trình có thể tối ưu các phép tính bằng ma trận:\n",
    "$$X^T*\\theta$$\n",
    "trong đó X là tham số đầu vào, $\\theta^{(1)}$ là ma trận các trọng số của lớp (1). (các hàng là trọng số tương ứng với đường đi từ neural i của lớp này đến neural j của lớp kia, có bao nhiêu neural ở lớp tiếp theo thì có bây nhiêu cột tương ứng.)\n",
    "\n",
    "Công thức tương ứng là:\n",
    "for l in {1..L}\n",
    "$$z^{(1)} = a^{(0)} * \\theta^{(1)}$$\n",
    "$$a^{(1)} = \\sigma.(z^{(1)})$$\n",
    "$$a^{(1)} = [a^{(1)},1]$$\n",
    "$$\\dots$$\n",
    "$$z^{(2)} = a^{(1)} * \\theta^{(2)}$$\n",
    "$$a^{(2)} = \\sigma.(z^{(2)})$$\n",
    "$$a^{(2)} = [a^{(2)},1]$$\n",
    "$$\\dots$$\n",
    "$$z^{(l)} = a^{(l-1)} * \\theta^{(l)}$$\n",
    "$$a^{(l)} = \\sigma.(z^{(l)})$$\n",
    "$$a^{(l)} = [a^{(l)},1]$$\n",
    "\n",
    "\n",
    "**Trong trường hợp ta cần đầu ra là một vector, ta có thể thêm, bớt, những neural để được kết quả tương ứng. Khi đó để vector này tuân theo một phân bố xác xuất, ta thêm một softmax activation ở cuối lớp**\n",
    "\n",
    "về nhà cài đặt MLDatasets và Flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
